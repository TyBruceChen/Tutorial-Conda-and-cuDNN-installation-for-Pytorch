# Configure pytorch for distributed GPU training on Ubuntu 18

In this tutorial, we'll try to realize distributed computing in DNN model training with ```torch.distributed``` (which is based on [NVIDIA Collective Communications Library (NCCL) function](https://developer.nvidia.com/nccl/nccl-legacy-downloads))
